{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abdullahnizami77/inshorts-scraping/blob/main/inshorts_scrapping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "import urllib\n",
        "import urllib.parse\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import quote\n",
        "import os\n",
        "import time\n",
        "import sys\n",
        "import random\n",
        "import string\n",
        "import re\n",
        "\n",
        "def myquote(quote_page):\n",
        "    url = urllib.parse.urlsplit(quote_page)\n",
        "    url = list(url)\n",
        "    url[2] = urllib.parse.quote(url[2])\n",
        "    url = urllib.parse.urlunsplit(url)\n",
        "    return url\n",
        "\n",
        "try:\n",
        "    if \"hindi\" in str(sys.argv[1]).lower():\n",
        "        file_name = \"/content/links.txt\"\n",
        "        destination_path = \"/content/dataset/hindi/\"\n",
        "        full_ds_path = \"/content/inshorts-dataset-hi/\"\n",
        "    else:\n",
        "        file_name = \"/content/links.txt\"\n",
        "        destination_path = \"/content/dataset/english/\"\n",
        "        full_ds_path = \"/content/inshorts-dataset-en/\"\n",
        "\n",
        "except:\n",
        "    print(\"\\tPlease provide 'Hindi' or 'English' after the command. Exit and run again.\")\n",
        "    input()\n",
        "    exit()\n",
        "\n",
        "os.makedirs(destination_path, exist_ok=True)\n",
        "os.makedirs(full_ds_path, exist_ok=True)\n",
        "\n",
        "linkfile = open(file_name, \"r\")\n",
        "links = linkfile.readlines()\n",
        "\n",
        "i = 0\n",
        "user_agent = 'Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.9.0.7) Gecko/2009021910 Firefox/3.0.7'\n",
        "headers = {'User-Agent': user_agent}\n",
        "k = 1\n",
        "\n",
        "display_top_n = 5\n",
        "\n",
        "for quote_page in links:\n",
        "    quote_page = quote_page.strip()\n",
        "\n",
        "    if quote_page.count('http') > 1:\n",
        "        continue\n",
        "\n",
        "    print(f\"Processing link: {quote_page}\")\n",
        "\n",
        "    try:\n",
        "        page = urllib.request.urlopen(myquote(quote_page))\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        input('wait1')\n",
        "        continue\n",
        "\n",
        "    if \"FILE_TERMINATES_HERE\" in quote_page:\n",
        "        break\n",
        "\n",
        "    rstring = ''.join(random.choices(string.ascii_uppercase, k=4))\n",
        "    dtstring = os.path.join(destination_path, re.search(r'\\d+$', quote_page)[0] + rstring + \".txt\")\n",
        "    dtstring2 = os.path.join(full_ds_path, re.search(r'\\d+$', quote_page)[0] + rstring + \".txt\")\n",
        "\n",
        "    file = open(dtstring, 'w')\n",
        "    article_len = 0\n",
        "\n",
        "    try:\n",
        "        soup = BeautifulSoup(page, 'html.parser')\n",
        "        selected_div = None\n",
        "        for div in soup.findAll('div'):\n",
        "            if div.text and 'read more at' in div.find(text=True):\n",
        "                selected_div = div\n",
        "                break\n",
        "        urlink = selected_div.find(\"a\", target=\"_blank\")['href']\n",
        "        request = urllib.request.Request(urlink, None, headers)\n",
        "        response = urllib.request.urlopen(request)\n",
        "        soup2 = BeautifulSoup(response, 'html.parser')\n",
        "        file.write(\"#originalArticleHeadline\" + \"\\n\")\n",
        "\n",
        "        if \"hindustantimes.com\" in urlink:\n",
        "            hlist = soup2.find('h1')\n",
        "            hlist2 = soup2.find('h2')\n",
        "            file.write(hlist.text + \"\\n\")\n",
        "            file.write(hlist2.text + \"\\n\")\n",
        "\n",
        "            file.write(\"#originalArticleBody\" + \"\\n\")\n",
        "            table = soup2.findAll('div', attrs={\"class\": \"story-details\"})\n",
        "            for x in table:\n",
        "                for p in x.findAll('p'):\n",
        "                    file.write(p.text + \"\\n\")\n",
        "\n",
        "        # I Will Add more website-specific parsing logic here as needed\n",
        "\n",
        "        else:\n",
        "            hlist = soup2.find('h1')\n",
        "            file.write(hlist.text + \"\\n\")\n",
        "            file.write(\"#originalArticleBody\" + \"\\n\")\n",
        "            article = soup2.findAll('p')\n",
        "            for a in article:\n",
        "                file.write(a.text + \"\\n\")\n",
        "    except Exception as e:\n",
        "        print('Abdullah Error', e)\n",
        "        #input('wait')\n",
        "        file.close()\n",
        "        os.remove(dtstring)\n",
        "        print('  --> Waiting of 1 sec. Press Ctrl+C to exit', dtstring, ' ' * 90, end='\\r')\n",
        "        time.sleep(1)\n",
        "        continue\n",
        "\n",
        "    file.write(\"\\n\")\n",
        "    file.write(\"-\" * 100 + \"\\n\")\n",
        "    file.write(\"#summaryHeadline\\n\" + soup.find(\"span\", itemprop=\"headline\").text.strip() + \"\\n\")\n",
        "    file.write(\"#summaryBody\\n\" + soup.find(\"div\", itemprop=\"articleBody\").text.strip() + \"\\n\")\n",
        "    file.write(\"#datePublished \" + soup.find(\"span\", itemprop=\"datePublished\").text.strip() + \" \")\n",
        "    file.write(soup.find(\"span\", clas=\"date\").text.strip() + \"\\n\")\n",
        "    #file.write(soup.find(\"span\", attrs={'class': 'short'}).text.strip() + \" by \" + soup.find(\"span\", attrs={\n",
        "        #'class': 'author'}).text.strip() + \" from News inShorts\\n\")\n",
        "    file.write(\"#reference_link: \" + quote_page + \"\\n\")\n",
        "    file.write(\"#original_link: \" + urlink)\n",
        "    file.close()\n",
        "\n",
        "    print('dtstring', dtstring)\n",
        "    #input('wait')\n",
        "\n",
        "    print(str(i) + \" \" + quote_page[29:-14] + \"\\n    Article and Summary pulled!\" + \" \" + datetime.datetime.now().strftime(\n",
        "        \"%H:%M:%S\") + ' ' * 100 + '\\n    file ' + dtstring2 + ' ' + \" \" * 100 + '\\n' + '-' * 50 + ' ' * 100)\n",
        "    if (k % display_top_n == 0):\n",
        "        print(\"\\033[A\" * (display_top_n * 4), end='')\n",
        "\n",
        "    k += 1\n",
        "\n",
        "print(\"\\n\" * 7 + str(k - 1) + \" Articles and their summaries pulled!\")\n"
      ],
      "metadata": {
        "id": "P83eh983zse8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2p-Gge3N1__z"
      },
      "source": [
        "# Exploratory Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "frC0_OIs2Ei_"
      },
      "outputs": [],
      "source": [
        "import urllib.request\n",
        "import urllib\n",
        "import urllib.parse\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import quote\n",
        "import datetime\n",
        "import os\n",
        "import time\n",
        "import sys\n",
        "import random, string, re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_hindi_url(quote_page):\n",
        "    url = urllib.parse.urlsplit(quote_page)\n",
        "    url = list(url)\n",
        "    url[2] = urllib.parse.quote(url[2])\n",
        "    url = urllib.parse.urlunsplit(url)\n",
        "    return url\n",
        "\n",
        "input_file = \"links.txt\"\n",
        "\n",
        "try:\n",
        "    file = open(input_file, \"r\", encoding=\"utf-8\")\n",
        "    links = file.readlines()\n",
        "    file.close()\n",
        "except FileNotFoundError:\n",
        "    print(f\"File '{input_file}' not found.\")\n",
        "    exit()\n",
        "\n",
        "\n",
        "output_directory = \"scraped_headings/\"\n",
        "\n",
        "\n",
        "if not os.path.exists(output_directory):\n",
        "    os.makedirs(output_directory)\n",
        "\n",
        "\n",
        "for i, link in enumerate(links[:5]):  # Only top 5 links\n",
        "    link = link.strip()\n",
        "    print(f\"Scraping link {i + 1}: {link}\")\n",
        "\n",
        "\n",
        "    encoded_link = encode_hindi_url(link)\n",
        "\n",
        "    try:\n",
        "        page = urllib.request.urlopen(encoded_link)\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching {encoded_link}: {e}\")\n",
        "        continue\n",
        "\n",
        "    soup = BeautifulSoup(page, 'html.parser')\n",
        "\n",
        "\n",
        "    heading = soup.find(\"span\", itemprop=\"headline\")\n",
        "\n",
        "    if heading:\n",
        "        heading_text = heading.text.strip()\n",
        "\n",
        "        filename = os.path.join(output_directory, f\"heading_{i + 1}.txt\")\n",
        "\n",
        "        with open(filename, \"w\", encoding=\"utf-8\") as file:\n",
        "            file.write(heading_text)\n",
        "\n",
        "        print(f\"Heading {i + 1} saved.\")\n",
        "\n",
        "    time.sleep(random.uniform(1, 3))\n",
        "\n",
        "print(f\"Scraping complete for the top 5 links. Headings saved in '{output_directory}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n2k7MWNPrQeA",
        "outputId": "65b277f7-3b0a-4631-e5e3-373abe59113c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping link 1: https://inshorts.com/hi/news/डेरा-सच्चा-सौदा-प्रमुख-गुरमीत-राम-रहीम-सिंह-अनुयायी-की-हत्या-के-लिए-दोषी-करार-1633677650409\n",
            "Heading 1 saved.\n",
            "Scraping link 2: https://inshorts.com/hi/news/सहारनपुर-में-बसपा-के-फज़लुर्रहमान-सबसे-आगे-मसूद-को-भी-मिल-चुके-सवा-लाख-वोट-1558601614502\n",
            "Heading 2 saved.\n",
            "Scraping link 3: https://inshorts.com/hi/news/गुरुग्राम-में-फ्लैट-के-नाम-पर-दिल्ली-के-व्यापारी-से-₹9125-की-ठगी-1556099842849\n",
            "Heading 3 saved.\n",
            "Scraping link 4: https://inshorts.com/hi/news/अपने-आत्मीय-मित्र-को-याद-कर-रहा-हूं-पासवान-की-बरसी-पर-उनके-परिवार-को-संदेश-में-पीएम-1631439024472\n",
            "Heading 4 saved.\n",
            "Scraping link 5: https://inshorts.com/hi/news/मुज़फ्फरनगर-में-प्रेम-प्रसंग-के-चलते-हमलावरों-ने-की-युवक-की-गोली-मारकर-हत्या-1559907245740\n",
            "Heading 5 saved.\n",
            "Scraping complete for the top 5 links. Headings saved in 'scraped_headings/'\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPzNItkmHoYbkDiQtmqRVOl",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}